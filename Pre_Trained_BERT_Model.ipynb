{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6CgeVzxpamND",
    "outputId": "3a92fd62-f2f6-461b-ed70-9cb8809d5818"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VfUhmV99Bmpk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "colab_type": "code",
    "id": "TzWQPnb9yh7u",
    "outputId": "b0fe30ca-e9e9-4f6c-e3fd-a24d727d649e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from bert import bert_tokenization as tokenization\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "f_chars_file = \"Coherence Model/F_text.txt\"\n",
    "f_chars_id = []\n",
    "with open(f_chars_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split(\" +++$+++ \")\n",
    "        k = line[0]\n",
    "        f_chars_id.append(k)\n",
    "\n",
    "m_chars_file = \"Coherence Model/M_text.txt\"\n",
    "m_chars_id = []\n",
    "with open(m_chars_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split(\" +++$+++ \")\n",
    "        k = line[0]\n",
    "        m_chars_id.append(k)\n",
    "\n",
    "f_lines = []\n",
    "m_lines = []\n",
    "lines_file = \"Colab Notebooks/lines.txt\"\n",
    "\n",
    "with open(lines_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split(\" +++$+++ \")\n",
    "        k = line[1]\n",
    "        v = line[4].strip()\n",
    "        if len(v.split()) > 128:\n",
    "            continue\n",
    "        if k in f_chars_id:\n",
    "            f_lines.append(v.lower())\n",
    "        elif k in m_chars_id:\n",
    "            m_lines.append(v.lower())\n",
    "\n",
    "# Load Pre-Trained BERT Model via TF 2.0\n",
    "# Prepare Data\n",
    "lines = m_lines + f_lines\n",
    "size = len(lines)\n",
    "max_seq_length = 130\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "train_data_seq = np.array([])\n",
    "train_data_word = np.array([])\n",
    "train_label = np.array([])\n",
    "\n",
    "for i in range(size):\n",
    "    line = lines[i]\n",
    "    if line in f_lines:\n",
    "        train_label = np.append(train_label, [1])\n",
    "    elif line in m_lines:\n",
    "        train_label = np.append(train_label, [0])\n",
    "\n",
    "    token = tokenizer.tokenize(line)\n",
    "    token = [\"[CLS]\"] + token + [\"[SEP]\"]\n",
    "\n",
    "    input_id = get_ids(token, tokenizer, max_seq_length)\n",
    "    input_mask = get_masks(token, max_seq_length)\n",
    "    input_segment = get_segments(token, max_seq_length)\n",
    "    \n",
    "    seq_data, word_data = model.predict([[input_id],[input_mask],[input_segment]])\n",
    "    train_data_seq = np.append(train_data_seq, seq_data)\n",
    "    train_data_word = np.append(train_data_word, word_data)\n",
    "\n",
    "train_data_seq = np.array(train_data_seq)\n",
    "train_data_word = np.array(train_data_word)\n",
    "train_labels = np.array(train_label)\n",
    "np.save(\"data_seq.npy\", train_data_seq)\n",
    "np.save(\"data_word.npy\", train_data_word)\n",
    "np.save(\"label.npy\", train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8W-3da0LFlWo",
    "outputId": "fdea6706-3540-4aee-cc28-4a7914cb4aa5"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fY7L7qb55YbB",
    "outputId": "c9e08988-a7f8-45d5-c37e-d5bec731429c"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "ZYbFdOSS0yS7",
    "outputId": "10a6c58f-9869-4d96-caa0-9418f205ed40"
   },
   "outputs": [],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "ksEP85COOuiH",
    "outputId": "d64a64b2-3860-408c-b9f7-9a9d9833187d"
   },
   "outputs": [],
   "source": [
    "# import bert\n",
    "# from bert import modeling\n",
    "import tensorflow as tf\n",
    "from bert import bert_tokenization as tokenization\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import nltk.tokenize as tk\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M-_WNRSvifCL"
   },
   "outputs": [],
   "source": [
    "f_chars_file = \"Coherence Model/F_text.txt\"\n",
    "f_chars_id = []\n",
    "with open(f_chars_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split(\" +++$+++ \")\n",
    "        k = line[0]\n",
    "        f_chars_id.append(k)\n",
    "\n",
    "m_chars_file = \"Coherence Model/M_text.txt\"\n",
    "m_chars_id = []\n",
    "with open(m_chars_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split(\" +++$+++ \")\n",
    "        k = line[0]\n",
    "        m_chars_id.append(k)\n",
    "\n",
    "f_lines = []\n",
    "m_lines = []\n",
    "lines_file = \"Colab Notebooks/lines.txt\"\n",
    "# data_clean = lines_file.read().replace('\\xad', '')\n",
    "with open(lines_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split(\" +++$+++ \")\n",
    "        k = line[1]\n",
    "        v = line[4].strip()\n",
    "        if len(v.split()) > 128:\n",
    "            continue\n",
    "        if k in f_chars_id:\n",
    "            f_lines.append(v.lower())\n",
    "        elif k in m_chars_id:\n",
    "            m_lines.append(v.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M_Xp_UuKyncg"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 128  \n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "h_RjCeoU88en"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "suiSLAzE80br"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9QO_ID5u0Sa0"
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kK62Kh36lWXh"
   },
   "outputs": [],
   "source": [
    "lines = m_lines + f_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "J20N5OE6nKJr"
   },
   "outputs": [],
   "source": [
    "size = len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XkWpNx3ClQqg"
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_label = []\n",
    "max_seq_length = 128\n",
    "unit = int(size / 5)\n",
    "test_size = size - unit * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0kwg4HcK8fpY"
   },
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LCJ1_BmC8v-O"
   },
   "outputs": [],
   "source": [
    "s = \"This is a nice sentence.\"\n",
    "stokens = tokenizer.tokenize(s)\n",
    "stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "input_masks = get_masks(stokens, max_seq_length)\n",
    "input_segments = get_segments(stokens, max_seq_length)\n",
    "\n",
    "pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YsEJ9dMU-ngV",
    "outputId": "b0e3eb54-0046-43ed-8580-2f5618f1c3e2"
   },
   "outputs": [],
   "source": [
    "a = np.array([1])\n",
    "np.append(a, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "colab_type": "code",
    "id": "Rn7BIHpm944U",
    "outputId": "8fec5d6d-e453-4016-dac2-06b6b2930e4d"
   },
   "outputs": [],
   "source": [
    "train_data_seq = np.array([])\n",
    "train_data_word = np.array([])\n",
    "train_label = np.array([])\n",
    "\n",
    "for i in range(size):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"No.\", i / 1000)\n",
    "    line = lines[i]\n",
    "    if line in f_lines:\n",
    "        train_label = np.append(train_label, [1])\n",
    "    elif line in m_lines:\n",
    "        train_label = np.append(train_label, [0])\n",
    "  \n",
    "    token = tokenizer.tokenize(line)\n",
    "    token = [\"[CLS]\"] + token + [\"[SEP]\"]\n",
    "\n",
    "    input_id = get_ids(token, tokenizer, max_seq_length)\n",
    "    input_mask = get_masks(token, max_seq_length)\n",
    "    input_segment = get_segments(token, max_seq_length)\n",
    "\n",
    "    seq_data, word_data = model.predict([[input_id],[input_mask],[input_segment]])\n",
    "  \n",
    "    if i == 0 :\n",
    "        train_data_seq = seq_data\n",
    "        train_data_word = word_data\n",
    "    else:\n",
    "        train_data_seq = np.append(train_data_seq, seq_data, axis = 0)\n",
    "        train_data_word = np.append(train_data_word, word_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PO60wcgU9JTe",
    "outputId": "69fb9586-ffbe-47cf-d3cb-227f7d17f190"
   },
   "outputs": [],
   "source": [
    "len(train_data_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6oa1L7h_9LzJ",
    "outputId": "2d58d951-1a6f-4e2c-891c-8b82371c544b"
   },
   "outputs": [],
   "source": [
    "all_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "j2cSratjS0Wn"
   },
   "outputs": [],
   "source": [
    "# BERT configures\n",
    "bert_config = modeling.BertConfig.from_json_file(\"/content/drive/My Drive/BERT_uncased_model/bert_config.json\")\n",
    "\n",
    "# Create BERT's input\n",
    "input_ids = tf.placeholder(shape=[None, max_seq_length], dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = tf.placeholder(shape=[None, max_seq_length], dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.placeholder(shape=[None, max_seq_length], dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "# Build BERT Model\n",
    "model = modeling.BertModel(\n",
    "    config = bert_config,\n",
    "    is_training = True,\n",
    "    input_ids = input_ids,\n",
    "    input_mask = input_mask,\n",
    "    token_type_ids = segment_ids,\n",
    "    use_one_hot_embeddings = False \n",
    ")\n",
    "\n",
    "init_checkpoint = \"/content/drive/My Drive/BERT_uncased_model/bert_model.ckpt\"\n",
    "use_tpu = False\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "(assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars,\n",
    "                                                                                       init_checkpoint)\n",
    "\n",
    "tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "encoder_last2_layer = model.all_encoder_layers[-2]\n",
    "encoder_last3_layer = model.all_encoder_layers[-3]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    for i in range(0, size):\n",
    "        text = lines[i]\n",
    "        queries = tk.sent_tokenize(text)\n",
    "        embeddings = []\n",
    "        for query in queries:\n",
    "            tokens = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            split_tokens = tokenizer.tokenize(query)\n",
    "            for token in split_tokens:\n",
    "                tokens.append(token)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            word_mask = [1] * len(word_ids)\n",
    "            word_segment_ids = [0] * len(word_ids)\n",
    "        while len(word_ids) < max_seq_length:\n",
    "            word_ids.append(0)\n",
    "            word_mask.append(0)\n",
    "            word_segment_ids.append(0)\n",
    "        fd = {input_ids: [word_ids], input_mask: [word_mask], segment_ids: [word_segment_ids]}\n",
    "\n",
    "        last2, last3 = sess.run([encoder_last2_layer, encoder_last3_layer], feed_dict=fd)\n",
    "\n",
    "        embedding = last2[0] + last3[0]\n",
    "        embedding = np.sum(embedding, axis=0)\n",
    "        embedding = embedding / max_seq_length\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "      # train_data dimension1: # essay \n",
    "      # train_data dimension2: # essay sentence\n",
    "      # train_data dimension3: the size of longest sentence (word)：192\n",
    "      # train_data dimension4：dimension of embedding ：768（pre-trained model：12-layer, 768-hidden, 12-heads and 110M parameters.）\n",
    "      \n",
    "        train_data.append(embeddings)\n",
    "        if lines[i] in f_lines:\n",
    "            train_label.append(0)\n",
    "        else:\n",
    "            train_label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qgoJGjr4p-EI"
   },
   "outputs": [],
   "source": [
    "train_dataset = np.array(train_data)\n",
    "train_labels = np.array(train_label)\n",
    "np.save(\"data.npy\", train_dataset) \n",
    "np.save(\"label.npy\", train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jxuMlDwntapL"
   },
   "outputs": [],
   "source": [
    "text = set_data['essay'][1114]\n",
    "query = tk.sent_tokenize(text)\n",
    "for q in query:\n",
    "    print (q)\n",
    "    print (len(tokenizer.tokenize(q)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pre-Trained BERT Model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
