{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Cc7RnCyn7lNM",
    "outputId": "dfcbe3cc-3f34-40dd-8ef5-98edbb826e20"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YgVeRtBloSxm",
    "outputId": "4dc95f68-d3a1-478d-97d9-e8b7d1f3eb6f"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZZ55R8KQ7XVz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "os.chdir('/content/drive/My Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEmgv-ovjKBx"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "z9cGIhYfdwp3"
   },
   "outputs": [],
   "source": [
    "gold_contents = np.load('/content/drive/My Drive/Semantic Model/word embedding/set1_data.npy')\n",
    "gold_labels = np.load('/content/drive/My Drive/Semantic Model/word embedding/set1_label.npy')\n",
    "\n",
    "# Fake data\n",
    "fake_contents = gold_contents\n",
    "for i in range(len(gold_contents)):\n",
    "    cont_size = len(gold_contents[i])\n",
    "    for j in range(cont_size):\n",
    "    fake_contents[i][j] = gold_contents[i][cont_size-1-j]\n",
    "fake_labels = np.array([0]*len(gold_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HpHSDAZjH77o"
   },
   "outputs": [],
   "source": [
    "# Normalize score\n",
    "min_val = 0\n",
    "max_val = max(gold_labels)\n",
    "gold_labels = (gold_labels - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5Bo_IxOBxKB-"
   },
   "outputs": [],
   "source": [
    "# Generate sequence length --> bi-lstm sequence lengthï¼ˆ\n",
    "gold_lengths = []\n",
    "for i in range(len(gold_contents)):\n",
    "    gold_lengths.append(len(gold_contents[i]))\n",
    "\n",
    "gold_lengths = np.array(gold_lengths)\n",
    "fake_lengths = gold_lengths\n",
    "max_length = max(gold_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5qFdcoY9AbAb"
   },
   "outputs": [],
   "source": [
    "# Extend train_content, test_content according to the max length of essay\n",
    "tmp = [0] * 768\n",
    "for i in range(len(gold_contents)):\n",
    "    length = len(gold_contents[i])\n",
    "    if length < max_length:\n",
    "        tmp_ = np.tile(tmp, (max_length - length, 1))\n",
    "        gold_contents[i] = np.append(gold_contents[i], tmp_, axis=0)\n",
    "for i in range(len(fake_contents)):\n",
    "    length = len(fake_contents[i])\n",
    "    if length < max_length:\n",
    "        tmp_ = np.tile(tmp, (max_length - length, 1))\n",
    "        fake_contents[i] = np.append(fake_contents[i], tmp_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Fl6qay1XQeXE"
   },
   "outputs": [],
   "source": [
    "gold_contents = np.load('/content/drive/My Drive/Coherence Model/data/gold data/gold_contents.npy')\n",
    "fake_contents = np.load('/content/drive/My Drive/Coherence Model/data/gold data/fake_contents.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qzCxsQhIWMLI"
   },
   "outputs": [],
   "source": [
    "np.save('/content/drive/My Drive/Coherence Model/data/gold data/gold_contents.npy', gold_data)\n",
    "np.save('/content/drive/My Drive/Coherence Model/data/gold data/fake_contents.npy', fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZO0Qqh9kkbNT"
   },
   "outputs": [],
   "source": [
    "gold_contents = np.load('/content/drive/My Drive/Coherence Model/data/gold data/gold_contents.npy')\n",
    "fake_contents = np.load('/content/drive/My Drive/Coherence Model/data/gold data/fake_contents.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "aCBcgKkNJVZR",
    "outputId": "7ef24d5d-1b6f-40e5-9028-f86bc8aa0c24"
   },
   "outputs": [],
   "source": [
    "min_size = 128\n",
    "for i in range(2,13):\n",
    "    print (\"****\", i)\n",
    "    curr_size = np.sum(gold_labels*(max_val-min_val)+min_val==i)\n",
    "    idxs = np.where((gold_labels*(max_val-min_val)+min_val==i)==True)\n",
    "\n",
    "    if curr_size >= min_size:\n",
    "        print (\"pass\")\n",
    "        continue  \n",
    "\n",
    "    else:\n",
    "        exp = min_size // curr_size\n",
    "        if (exp - 1 > 0):\n",
    "            tmp_data = np.tile(gold_contents[idxs], (exp-1,1,1))\n",
    "            tmp_labels = np.tile(gold_labels[idxs], exp-1)\n",
    "            tmp_lengths = np.tile(gold_lengths[idxs], exp-1)\n",
    "\n",
    "            gold_contents = np.concatenate((gold_contents, tmp_data), axis = 0)\n",
    "            gold_labels = np.concatenate((gold_labels, tmp_labels), axis = 0)\n",
    "            gold_lengths = np.concatenate((gold_lengths, tmp_lengths), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTX2rLjkfw5d"
   },
   "source": [
    "5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "d20aWoHofv0W"
   },
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "permutation = np.random.permutation(fake_labels.shape[0])[0: min_size]\n",
    "shuffled_fake_content = fake_contents[permutation, :, :]\n",
    "shuffled_fake_labels = fake_labels[permutation]\n",
    "shuffled_fake_length = fake_lengths[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RV0rJfV-ORMj"
   },
   "outputs": [],
   "source": [
    "shuffled_content = np.concatenate((gold_contents, shuffled_fake_content), axis = 0)\n",
    "shuffled_labels = np.concatenate((gold_labels, shuffled_fake_labels), axis = 0)\n",
    "shuffled_length = np.concatenate((gold_lengths, shuffled_fake_length), axis = 0)\n",
    "\n",
    "permutation = np.random.permutation(shuffled_labels.shape[0])\n",
    "shuffled_content = shuffled_content[permutation, :, :]\n",
    "shuffled_labels = shuffled_labels[permutation]\n",
    "shuffled_length = shuffled_length[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oMjx69T5zZno"
   },
   "outputs": [],
   "source": [
    "# Calculate rate\n",
    "label_rate = []\n",
    "for i in range(0, 13):\n",
    "    label_rate.append(np.sum(shuffled_labels*12==i))\n",
    "label_rate = np.array(label_rate)\n",
    "label_rate = label_rate / (len(shuffled_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yMAFU_wMheTO"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    dataset = np.concatenate((shuffled_content[0 : 5*unit], shuffled_content[0 : 4*unit]), axis=0) \n",
    "    label = np.concatenate((shuffled_labels[0 : 5*unit], shuffled_labels[0 : 4*unit]), axis=0) \n",
    "    length = np.concatenate((shuffled_length[0 : 5*unit], shuffled_length[0 : 4*unit]), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FSH9tNc35hX2"
   },
   "outputs": [],
   "source": [
    "dataset = np.load('/content/drive/My Drive/Coherence Model/data/dataset.npy')\n",
    "label = np.load('/content/drive/My Drive/Coherence Model/data/label.npy')\n",
    "length = np.load('/content/drive/My Drive/Coherence Model/data/length.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zb8b4vHZlhig"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "size = len(label)\n",
    "unit = int(size / 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4_u_F8Ha79-R"
   },
   "outputs": [],
   "source": [
    "label_rates = [0] * len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PR3WntEGYGOF"
   },
   "outputs": [],
   "source": [
    "# 5 setsï¼šeach set --> 60% train datasetï¼Œ20% development datasetï¼Œ20% test dataset\n",
    "train_dataset = []\n",
    "train_label = []\n",
    "train_length = []\n",
    "train_rate = []\n",
    "\n",
    "dev_dataset = []\n",
    "dev_label = []\n",
    "dev_length = []\n",
    "dev_rate = []\n",
    "\n",
    "test_dataset = []\n",
    "test_label = []\n",
    "test_length = []\n",
    "test_rate = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_dataset.append([dataset[i*unit : (i+3)*unit]])\n",
    "    train_label.append([label[i*unit : (i+3)*unit]])\n",
    "    train_length.append([length[i*unit : (i+3)*unit]])\n",
    "    train_rate.append([label_rates[i*unit : (i+3)*unit]])\n",
    "    dev_dataset.append([dataset[(i+3)*unit : (i+4)*unit]])\n",
    "    dev_label.append([label[(i+3)*unit : (i+4)*unit]])\n",
    "    dev_length.append([length[(i+3)*unit : (i+4)*unit]])\n",
    "    dev_rate.append([label_rates[(i+3)*unit : (i+4)*unit]])\n",
    "    test_dataset.append([dataset[(i+4)*unit : (i+5)*unit]])\n",
    "    test_label.append([label[(i+4)*unit : (i+5)*unit]])\n",
    "    test_length.append([length[(i+4)*unit : (i+5)*unit]])\n",
    "    test_rate.append([label_rates[(i+4)*unit : (i+5)*unit]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8XvByPNqL0x"
   },
   "source": [
    "Generate training dataset with batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1C8ww2XRqLVU"
   },
   "outputs": [],
   "source": [
    "def batch_iter(gold_data, gold_labels, gold_lengths, \n",
    "               gold_rate, batch_size, num_epochs):\n",
    "\n",
    "    assert len(gold_data) == len(gold_labels) == len(gold_lengths)# == len(fake_data) == len(fake_labels) == len(fake_lengths)\n",
    "    data_size = len(gold_data)\n",
    "\n",
    "    epoch_length = data_size // batch_size\n",
    "  \n",
    "    for _ in range(num_epochs):\n",
    "        for i in range(epoch_length):\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "            xdata = gold_data[start_index: end_index]\n",
    "            ydata = gold_labels[start_index: end_index]\n",
    "            sequence_length = gold_lengths[start_index: end_index]\n",
    "            rate = gold_rate[start_index: end_index]\n",
    "\n",
    "            yield xdata, ydata, sequence_length, rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOt2oZvs_TJd"
   },
   "source": [
    "Bi-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wsrtT6In7tFM"
   },
   "outputs": [],
   "source": [
    "class Bi_LSTM_Model(object):\n",
    "    def __init__(self):\n",
    "        # Score rangeï¼š 2ï½ž12 \n",
    "        self.num_classes = 13\n",
    "        self.hidden_size = 1024\n",
    "        self.num_layers = 1\n",
    "        self.l2_reg_lambda = 0.005\n",
    "        \n",
    "        # Placeholders\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[], name='batch_size')\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, 68, 768], name='input_x')\n",
    "        self.input_y = tf.placeholder(dtype=tf.int64, shape=[256], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='keep_prob')\n",
    "        self.sequence_length = tf.placeholder(dtype=tf.int32, shape=[None], name='sequence_length')\n",
    "        self.rate = tf.placeholder(dtype=tf.float32, shape=[256], name='rate')\n",
    "        \n",
    "        # L2 loss\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Word embedding\n",
    "        with tf.name_scope('embedding'):\n",
    "            inputs = self.input_x\n",
    "\n",
    "\n",
    "        # Input dropout\n",
    "        self.inputs = tf.nn.dropout(inputs, keep_prob=self.keep_prob)\n",
    "\n",
    "        self.final_state = self.bi_lstm()\n",
    "        \n",
    "        # Softmax output layer\n",
    "        with tf.name_scope('sigmoid'):\n",
    "     \n",
    "            sigmoid_w = tf.get_variable('sigmoid_w', shape=[2 * self.hidden_size, 13], dtype=tf.float32)\n",
    "            sigmoid_b = tf.get_variable('sigmoid_b', shape=[13])\n",
    "\n",
    "            # L2 regularization for output layer\n",
    "            self.l2_loss += tf.nn.l2_loss(sigmoid_w)\n",
    "            self.l2_loss += tf.nn.l2_loss(sigmoid_b)\n",
    "            \n",
    "            self.logits = tf.matmul(self.final_state, sigmoid_w) + sigmoid_b\n",
    "            predictions = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(predictions, 1, name='predictions')\n",
    "#             self.predictions = tf.nn.sigmoid(tf.matmul(self.final_state, sigmoid_w) + sigmoid_b, name='predictions')\n",
    "\n",
    "        # Loss\n",
    "        with tf.name_scope('loss'):\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            # L2 regularization for LSTM weights\n",
    "            for tv in tvars:\n",
    "                if 'kernel' in tv.name:\n",
    "                    self.l2_loss += tf.nn.l2_loss(tv)\n",
    "#             losses = tf.square((self.predictions - self.input_y) / self.rate)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,\n",
    "                                                                    logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(losses, name=\"loss\") + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "#             correct_predictions = tf.equal(tf.round(self.predictions*(max_val-min_val)+min_val), self.input_y*(max_val-min_val)+min_val)\n",
    "            \n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "\n",
    "\n",
    "    def bi_lstm(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                          forget_bias=1.0,\n",
    "                                          state_is_tuple=True,\n",
    "                                          reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                          forget_bias=1.0,\n",
    "                                          state_is_tuple=True,\n",
    "                                          reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "        # Add dropout to cell output\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.keep_prob)\n",
    "        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.keep_prob)\n",
    "\n",
    "        # Stacked LSTMs\n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell([cell_fw], state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell([cell_bw], state_is_tuple=True)\n",
    "\n",
    "        self._initial_state_fw = cell_fw.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self._initial_state_bw = cell_bw.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        # Dynamic Bi-LSTM\n",
    "        with tf.variable_scope('Bi-LSTM'):\n",
    "            _, state = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "                                                       cell_bw,\n",
    "                                                       inputs=self.inputs,\n",
    "                                                       initial_state_fw=self._initial_state_fw,\n",
    "                                                       initial_state_bw=self._initial_state_bw,\n",
    "                                                       sequence_length=self.sequence_length)\n",
    "\n",
    "        state_fw = state[0]\n",
    "        state_bw = state[1]\n",
    "        output = tf.concat([state_fw[self.num_layers - 1].h, state_bw[self.num_layers - 1].h], 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsyRl-YFp7gf"
   },
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lUPHc9OHp8mm"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters (LSTMs are all single layer)\n",
    "hidden_size = 1024 # Number of hidden units in the LSTM cell\n",
    "keep_prob = 0.5 # Dropout keep probability\n",
    "learning_rate = 1e-5 \n",
    "l2_reg_lambda = 0.001 # L2 regularization lambda\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 256\n",
    "num_epochs = 100 \n",
    "decay_rate = 1 \n",
    "decay_steps = 100000 # Learning rate decay rate. Range: (0, 1]\n",
    "save_every_steps = 100\n",
    "evaluate_every_steps = 10 # Evaluate the model on validation set after this many steps\n",
    "num_checkpoint = 50 # number of models to store\n",
    "\n",
    "for i in range(4,5):\n",
    "    outdir = os.path.abspath(os.path.join(os.path.curdir, \"Coherence Model\", \"runs\", \"Bi-LSTM\", \"Model_\" + str(i)))\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # Load and save data\n",
    "    # =============================================================================\n",
    "\n",
    "    # Simple Cross validation\n",
    "    # Batch iterator\n",
    "    x_gold_train = train_dataset[i][0]\n",
    "    y_gold_train = train_label[i][0]*12\n",
    "    x_gold_dev = dev_dataset[i][0]\n",
    "    y_gold_dev = dev_label[i][0]*12\n",
    "    train_gold_lengths = train_length[i][0]\n",
    "    dev_gold_lengths = dev_length[i][0]\n",
    "    train_gold_rate = train_rate[i][0]\n",
    "    dev_gold_rate = dev_rate[i][0]\n",
    " \n",
    "    train_data = batch_iter(x_gold_train, y_gold_train, train_gold_lengths, \n",
    "                          train_gold_rate, batch_size, num_epochs)\n",
    "  \n",
    "    # Train\n",
    "    # =============================================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            classifier = Bi_LSTM_Model()\n",
    "            # Train procedure\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            # Learning rate decay\n",
    "            starter_learning_rate = learning_rate\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                                     global_step,\n",
    "                                                     decay_steps,\n",
    "                                                     decay_rate,\n",
    "                                                     staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Summaries\n",
    "            loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "            accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "            # Train summary\n",
    "            train_summary_op = tf.summary.merge_all()\n",
    "            train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Validation summary\n",
    "            valid_summary_op = tf.summary.merge_all()\n",
    "            valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "            valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "            saver = tf.train.Saver(max_to_keep=num_checkpoint)\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "            def run_step(input_data, is_training=True):\n",
    "            \"\"\"Run one step of the training process.\"\"\"\n",
    "                input_x, input_y, sequence_length, input_rate = input_data\n",
    "\n",
    "                fetches = {'step': global_step,\n",
    "                         'cost': classifier.cost,\n",
    "                         'accuracy': classifier.accuracy,\n",
    "                         'learning_rate': learning_rate}\n",
    "                feed_dict = {classifier.input_x: input_x,\n",
    "                           classifier.input_y: input_y,\n",
    "                          classifier.rate: input_rate}\n",
    "                fetches['final_state'] = classifier.final_state\n",
    "                feed_dict[classifier.batch_size] = len(input_x)\n",
    "                feed_dict[classifier.sequence_length] = sequence_length\n",
    "\n",
    "                if is_training:\n",
    "                    fetches['train_op'] = train_op\n",
    "                    fetches['summaries'] = train_summary_op\n",
    "                    feed_dict[classifier.keep_prob] = keep_prob\n",
    "                else:\n",
    "                    fetches['summaries'] = valid_summary_op\n",
    "                    feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "                vars = sess.run(fetches, feed_dict)\n",
    "                step = vars['step']\n",
    "                cost = vars['cost']\n",
    "                accuracy = vars['accuracy']\n",
    "                summaries = vars['summaries']\n",
    "\n",
    "                # Write summaries to file\n",
    "                if is_training:\n",
    "                    train_summary_writer.add_summary(summaries, step)\n",
    "                else:\n",
    "                    valid_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step: {}, loss: {:g}, accuracy: {:g}\".format(time_str, step, cost, accuracy))\n",
    "\n",
    "                return accuracy\n",
    "\n",
    "\n",
    "            print('Start training ...')\n",
    "\n",
    "            for train_input in train_data:\n",
    "                run_step(train_input, is_training=True)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                \n",
    "                if current_step % evaluate_every_steps == 0:\n",
    "                    print('\\nDevlopment Set Validation')\n",
    "                    dev_data = batch_iter(x_gold_dev, y_gold_dev, dev_gold_lengths, \n",
    "                                        dev_gold_rate,\n",
    "                                        batch_size, 1)\n",
    "                    for dev_input in dev_data:\n",
    "                        run_step(dev_input, is_training=False)\n",
    "                    print('End Development Set Validation\\n')\n",
    "\n",
    "                if current_step % save_every_steps == 0:\n",
    "                    save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "\n",
    "            print('\\nAll the files have been saved to {}\\n'.format(outdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PeqBlamCqOrG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/ASAPAES')\n",
    "import score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "boHbyC9gEwCm"
   },
   "source": [
    "Load Bi-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "6L1lQIIwdCyK",
    "outputId": "2ec252b3-801b-483b-92c6-e3248414512f"
   },
   "outputs": [],
   "source": [
    "inference_graph = tf.Graph()\n",
    "with tf.Session(graph = inference_graph) as sess:\n",
    "  \n",
    "    graph = tf.get_default_graph()\n",
    "    path = '/content/drive/My Drive/Coherence Model/runs/Bi-LSTM/'\n",
    "\n",
    "  \n",
    "    for i in range(2,3):\n",
    "        print(\"No.\" + str(i) + \" Model\\n\")\n",
    "        bilstm_model = tf.train.import_meta_graph(os.path.join(path, \"Model_\"+str(i), \"model\", \"clf-500.meta\"))\n",
    "        bilstm_model.restore(sess, tf.train.latest_checkpoint(os.path.join(path, \"Model_\"+str(i), \"model\")))\n",
    "    \n",
    "        x_dev = test_dataset[i][0]\n",
    "        y_dev = test_label[i][0]\n",
    "        dev_lengths = test_length[i][0]\n",
    "        dev_rates = test_rate[i][0]\n",
    "\n",
    "        print('\\nDevlopment Set Validation ' + str(i))\n",
    "        dev_data = batch_iter(x_dev, y_dev, dev_lengths, dev_rates,256, 1)\n",
    "        for dev_input in dev_data:\n",
    "      \n",
    "            x_ = inference_graph.get_tensor_by_name('input_x:0')\n",
    "            y_ = inference_graph.get_tensor_by_name('input_y:0')\n",
    "            rate_ = inference_graph.get_tensor_by_name('rate:0')\n",
    "            prediction_ = inference_graph.get_tensor_by_name('sigmoid/predictions:0')\n",
    "            keep_prob_ = inference_graph.get_tensor_by_name('keep_prob:0')\n",
    "            loss_ = inference_graph.get_tensor_by_name('loss/add_2:0')\n",
    "            accuracy_ = inference_graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "            sequence_length_ = inference_graph.get_tensor_by_name('sequence_length:0')\n",
    "            batch_size_ = inference_graph.get_tensor_by_name('batch_size:0')\n",
    "            vars = sess.run([accuracy_, loss_, prediction_], \n",
    "                      feed_dict={x_: dev_input[0],\n",
    "                                 y_: dev_input[1],\n",
    "                                 keep_prob_: 1.0,\n",
    "                                 sequence_length_: dev_input[2],\n",
    "                                 rate_: dev_input[3],\n",
    "                                 batch_size_ : 256})\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            qwks = score.quadratic_weighted_kappa(vars[2], (dev_input[1]*12).astype(int), 0, 12)\n",
    "            print(\"qwks\",qwks,\" \", score.mean_quadratic_weighted_kappa([qwks]))\n",
    "            acc = (np.sum(((vars[2]-dev_input[1]*12)==0)==True)/len(vars[2]))\n",
    "            print(\"{}: loss: {:g}, accuracy: {:g}\".format(time_str, vars[1], acc))\n",
    "            print('End Development Set Validation ' + str(i) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "F6-IvJakguX9",
    "outputId": "aa1c04e3-ddd6-4320-8775-a33b61fe20cf"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/benhamner/ASAP-AES.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGe3g0WGqNlD"
   },
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6tYoRzYlqMdW"
   },
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self):\n",
    "        # Score rangeï¼š 2ï½ž12 \n",
    "        self.num_classes = 13\n",
    "        self.hidden_size = 1024\n",
    "        self.num_layers = 1\n",
    "        self.l2_reg_lambda = 0.005\n",
    "        \n",
    "        # Placeholders\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[], name='batch_size')\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, 68, 768], name='input_x')\n",
    "        self.input_y = tf.placeholder(dtype=tf.int64, shape=[256], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='keep_prob')\n",
    "        self.sequence_length = tf.placeholder(dtype=tf.int32, shape=[None], name='sequence_length')\n",
    "        self.rate = tf.placeholder(dtype=tf.float32, shape=[256], name='rate')\n",
    "        # L2 loss\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Word embedding\n",
    "        with tf.name_scope('embedding'):\n",
    "#             embedding = tf.get_variable('embedding',\n",
    "#                                         shape=[self.vocab_size, self.hidden_size],\n",
    "#                                         dtype=tf.float32)\n",
    "#             inputs = tf.nn.embedding_lookup(wv, self.input_x)\n",
    "\n",
    "            inputs = self.input_x\n",
    "\n",
    "\n",
    "        # Input dropout\n",
    "        self.inputs = tf.nn.dropout(inputs, keep_prob=self.keep_prob)\n",
    "\n",
    "        self.final_state = self.lstm()\n",
    "        \n",
    "        # Softmax output layer\n",
    "        with tf.name_scope('sigmoid'):\n",
    "     \n",
    "            sigmoid_w = tf.get_variable('sigmoid_w', shape=[self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            sigmoid_b = tf.get_variable('sigmoid_b', shape=[self.num_classes])\n",
    "\n",
    "            # L2 regularization for output layer\n",
    "            self.l2_loss += tf.nn.l2_loss(sigmoid_w)\n",
    "            self.l2_loss += tf.nn.l2_loss(sigmoid_b)\n",
    "            \n",
    "            self.logits = tf.matmul(self.final_state, sigmoid_w) + sigmoid_b\n",
    "            predictions = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(predictions, 1, name='predictions')\n",
    "#             self.predictions = tf.nn.sigmoid(tf.matmul(self.final_state, sigmoid_w) + sigmoid_b, name='predictions')\n",
    "\n",
    "        # Loss (MSE)\n",
    "        with tf.name_scope('loss'):\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            # L2 regularization for LSTM weights\n",
    "            for tv in tvars:\n",
    "                if 'kernel' in tv.name:\n",
    "                    self.l2_loss += tf.nn.l2_loss(tv)\n",
    "#             losses = tf.square((self.predictions - self.input_y) / self.rate)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,\n",
    "                                                                    logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(losses, name=\"loss\") + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "#             correct_predictions = tf.equal(tf.round(self.predictions*(max_val-min_val)+min_val), self.input_y*(max_val-min_val)+min_val) \n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "\n",
    "    def lstm(self):\n",
    "\n",
    "        cell = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                          forget_bias= 1.0,\n",
    "                                          state_is_tuple=True,\n",
    "                                          reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "\n",
    "        # Add dropout to cell output\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "\n",
    "        # Stacked LSTMs\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell], state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        # Dynamic LSTM\n",
    "        with tf.variable_scope('LSTM'):\n",
    "            _, state = tf.nn.dynamic_rnn(cell,\n",
    "                                         inputs=self.inputs,\n",
    "                                         initial_state=self._initial_state,\n",
    "                                         sequence_length=self.sequence_length)\n",
    "\n",
    "        output = state[self.num_layers - 1].h\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "P7WQuVwIenpE"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters (LSTMs are all single layer)\n",
    "hidden_size = 1024 # Number of hidden units in the LSTM cell\n",
    "keep_prob = 0.5 # Dropout keep probability\n",
    "learning_rate = 1e-5 \n",
    "l2_reg_lambda = 0.001 # L2 regularization lambda\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128 * 2\n",
    "num_epochs = 100\n",
    "decay_rate = 1 \n",
    "decay_steps = 100000 # Learning rate decay rate. Range: (0, 1]\n",
    "save_every_steps = 100\n",
    "evaluate_every_steps = 10 # Evaluate the model on validation set after this many steps\n",
    "num_checkpoint = 50 # number of models to store\n",
    "\n",
    "for i in range(4,5):\n",
    "    outdir = os.path.abspath(os.path.join(os.path.curdir, \"Coherence Model\", \"runs\", \"LSTM\", \"Model_\"+str(i)))\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # Load and save data\n",
    "    # =============================================================================\n",
    "\n",
    "    # Simple Cross validation\n",
    "    # x_train, x_valid, y_train, y_valid, train_lengths, valid_lengths =  , test_content, train_labels, test_labels, train_length, test_length\n",
    "    # Batch iterator\n",
    "    x_gold_train = train_dataset[i][0]\n",
    "    y_gold_train = train_label[i][0]*12\n",
    "    x_gold_dev = dev_dataset[i][0]\n",
    "    y_gold_dev = dev_label[i][0]*12\n",
    "    train_gold_lengths = train_length[i][0]\n",
    "    dev_gold_lengths = dev_length[i][0]\n",
    "    train_gold_rate = train_rate[i][0]\n",
    "    dev_gold_rate = dev_rate[i][0]\n",
    " \n",
    "    train_data = batch_iter(x_gold_train, y_gold_train, train_gold_lengths, \n",
    "                          train_gold_rate, batch_size, num_epochs)\n",
    "  \n",
    "    # Train\n",
    "    # =============================================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            classifier = LSTM_Model()\n",
    "            # Train procedure\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            # Learning rate decay\n",
    "            starter_learning_rate = learning_rate\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                                     global_step,\n",
    "                                                     decay_steps,\n",
    "                                                     decay_rate,\n",
    "                                                     staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Summaries\n",
    "            loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "            accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "            # Train summary\n",
    "            train_summary_op = tf.summary.merge_all()\n",
    "            train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Validation summary\n",
    "            valid_summary_op = tf.summary.merge_all()\n",
    "            valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "            valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "            saver = tf.train.Saver(max_to_keep=num_checkpoint)\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "            def run_step(input_data, is_training=True):\n",
    "            \"\"\"Run one step of the training process.\"\"\"\n",
    "                input_x, input_y, sequence_length, input_rate = input_data\n",
    "\n",
    "                fetches = {'step': global_step,\n",
    "                         'cost': classifier.cost,\n",
    "                         'accuracy': classifier.accuracy,\n",
    "                         'learning_rate': learning_rate}\n",
    "                feed_dict = {classifier.input_x: input_x,\n",
    "                           classifier.input_y: input_y,\n",
    "                          classifier.rate: input_rate}\n",
    "                fetches['final_state'] = classifier.final_state\n",
    "                feed_dict[classifier.batch_size] = len(input_x)\n",
    "                feed_dict[classifier.sequence_length] = sequence_length\n",
    "\n",
    "                if is_training:\n",
    "                    fetches['train_op'] = train_op\n",
    "                    fetches['summaries'] = train_summary_op\n",
    "                    feed_dict[classifier.keep_prob] = keep_prob\n",
    "                else:\n",
    "                    fetches['summaries'] = valid_summary_op\n",
    "                    feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "                vars = sess.run(fetches, feed_dict)\n",
    "                step = vars['step']\n",
    "                cost = vars['cost']\n",
    "                accuracy = vars['accuracy']\n",
    "                summaries = vars['summaries']\n",
    "\n",
    "                # Write summaries to file\n",
    "                if is_training:\n",
    "                    train_summary_writer.add_summary(summaries, step)\n",
    "                else:\n",
    "                    valid_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step: {}, loss: {:g}, accuracy: {:g}\".format(time_str, step, cost, accuracy))\n",
    "\n",
    "                return accuracy\n",
    "\n",
    "\n",
    "            print('Start training ...')\n",
    "\n",
    "            for train_input in train_data:\n",
    "                run_step(train_input, is_training=True)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % evaluate_every_steps == 0:\n",
    "                    print('\\nDevlopment Set Validation')\n",
    "                    dev_data = batch_iter(x_gold_dev, y_gold_dev, dev_gold_lengths, dev_gold_rate,\n",
    "                                        batch_size, 1)\n",
    "                    for dev_input in dev_data:\n",
    "                        run_step(dev_input, is_training=False)\n",
    "                    print('End Development Set Validation\\n')\n",
    "\n",
    "                if current_step % save_every_steps == 0:\n",
    "                    save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "\n",
    "            print('\\nAll the files have been saved to {}\\n'.format(outdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "y3hq8-rgVqdR",
    "outputId": "1524d545-3b2d-48eb-edb3-b87fc8f0d815"
   },
   "outputs": [],
   "source": [
    "inference_graph = tf.Graph()\n",
    "with tf.Session(graph = inference_graph) as sess:\n",
    "  \n",
    "    graph = tf.get_default_graph()\n",
    "    path = '/content/drive/My Drive/Coherence Model/runs/LSTM/'\n",
    "\n",
    "    for i in range(2,3):\n",
    "        print(\"No.\" + str(i) + \" Model\\n\")\n",
    "        bilstm_model = tf.train.import_meta_graph(os.path.join(path, \"Model_\"+str(i), \"model\", \"clf-500.meta\"))\n",
    "        bilstm_model.restore(sess, tf.train.latest_checkpoint(os.path.join(path, \"Model_\"+str(i), \"model\")))\n",
    "        print ([n.name for n in graph.as_graph_def().node])\n",
    "    \n",
    "        x_dev = test_dataset[i][0]\n",
    "        y_dev = test_label[i][0]\n",
    "        dev_lengths = test_length[i][0]\n",
    "        dev_rates = test_rate[i][0]\n",
    "\n",
    "        print('\\nDevlopment Set Validation ' + str(i))\n",
    "        dev_data = batch_iter(x_dev, y_dev, dev_lengths, dev_rates,256, 1)\n",
    "        for dev_input in dev_data:\n",
    "      \n",
    "            x_ = inference_graph.get_tensor_by_name('input_x:0')\n",
    "            y_ = inference_graph.get_tensor_by_name('input_y:0')\n",
    "            rate_ = inference_graph.get_tensor_by_name('rate:0')\n",
    "            prediction_ = inference_graph.get_tensor_by_name('sigmoid/predictions:0')\n",
    "            keep_prob_ = inference_graph.get_tensor_by_name('keep_prob:0')\n",
    "            loss_ = inference_graph.get_tensor_by_name('loss/loss:0')\n",
    "            accuracy_ = inference_graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "            sequence_length_ = inference_graph.get_tensor_by_name('sequence_length:0')\n",
    "            batch_size_ = inference_graph.get_tensor_by_name('batch_size:0')\n",
    "            vars = sess.run([accuracy_, loss_, prediction_], \n",
    "                      feed_dict={x_: dev_input[0],\n",
    "                                 y_: dev_input[1],\n",
    "                                 keep_prob_: 1.0,\n",
    "                                 sequence_length_: dev_input[2],\n",
    "                                 rate_: dev_input[3],\n",
    "                                 batch_size_ : 256})\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            qwks = score.quadratic_weighted_kappa(vars[2], (dev_input[1]*12).astype(int), 0, 12)\n",
    "            print(\"qwks\",qwks,\" \", score.mean_quadratic_weighted_kappa([qwks]))\n",
    "            acc = (np.sum(((vars[2]-dev_input[1]*12)==0)==True)/len(vars[2]))\n",
    "            print(\"{}: loss: {:g}, accuracy: {:g}\".format(time_str, vars[1], acc))\n",
    "            print('End Development Set Validation ' + str(i) +'\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Network Coherence Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
