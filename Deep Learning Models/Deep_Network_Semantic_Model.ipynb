{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Cc7RnCyn7lNM",
    "outputId": "a59ade97-89d4-40db-9d59-c96f97b142a2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YgVeRtBloSxm",
    "outputId": "71e2316d-48f0-4ad7-89e4-9d84086122f1"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ZZ55R8KQ7XVz",
    "outputId": "a9de95e5-6b2a-4878-c729-076a11b43c36"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "os.chdir('/content/drive/My Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEmgv-ovjKBx"
   },
   "source": [
    "Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "z9cGIhYfdwp3"
   },
   "outputs": [],
   "source": [
    "# Sentence Embedding\n",
    "gold_contents = np.load('/content/drive/My Drive/Semantic Model/word embedding/data.npy')\n",
    "gold_labels = np.load('/content/drive/My Drive/Semantic Model/word embedding/label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tQuEW4qhOva9"
   },
   "outputs": [],
   "source": [
    "# Normalize score\n",
    "min_val = min(gold_labels)\n",
    "max_val = max(gold_labels)\n",
    "gold_labels = (gold_labels - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5Bo_IxOBxKB-"
   },
   "outputs": [],
   "source": [
    "# Generate sequence length --> bi-lstm sequence length\n",
    "gold_lengths = []\n",
    "for i in range(len(gold_contents)):\n",
    "    gold_lengths.append(len(gold_contents[i]))\n",
    "gold_lengths = np.array(gold_lengths)\n",
    "max_length = max(gold_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5qFdcoY9AbAb"
   },
   "outputs": [],
   "source": [
    "# Extend train_content, test_content according to the max length of essay\n",
    "tmp = [0] * 1024\n",
    "for i in range(len(gold_contents)):\n",
    "    length = len(gold_contents[i])\n",
    "    if length < max_length:\n",
    "        tmp_ = np.tile(tmp, (max_length - length, 1))\n",
    "        gold_contents[i] = np.append(gold_contents[i], tmp_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Fl6qay1XQeXE"
   },
   "outputs": [],
   "source": [
    "data = [gold_contents[0]]\n",
    "for i in range(1, len(gold_contents)):\n",
    "    data = np.concatenate((data, [gold_contents[i]]), axis=0)\n",
    "contents = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yhLUlyKFm1Fj",
    "outputId": "fb82c86a-49e2-4ecd-ffa1-f42fa4d294aa"
   },
   "outputs": [],
   "source": [
    "np.sum(gold_labels*(max_val-min_val)+min_val==7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yo6vVTcnl3dD",
    "outputId": "193bf761-2859-4e91-9ae8-a6e8ae804abf"
   },
   "outputs": [],
   "source": [
    "min_size = 128\n",
    "for i in range(2,13):\n",
    "    print (\"****\", i)\n",
    "    curr_size = np.sum(gold_labels*(max_val-min_val)+min_val==i)\n",
    "    print (curr_size)\n",
    "    idxs = np.where((gold_labels*(max_val-min_val)+min_val==i)==True)\n",
    "\n",
    "    if curr_size >= min_size:\n",
    "        print (\"pass\")\n",
    "        continue  \n",
    "\n",
    "    else:\n",
    "        exp = min_size // curr_size\n",
    "    \n",
    "        if (exp - 1 > 0):\n",
    "            tmp_data = np.tile(contents[idxs], (exp-1,1,1))\n",
    "            tmp_labels = np.tile(gold_labels[idxs], exp-1)\n",
    "            tmp_lengths = np.tile(gold_lengths[idxs], exp-1)\n",
    "\n",
    "            contents = np.concatenate((contents, tmp_data), axis = 0)\n",
    "            gold_labels = np.concatenate((gold_labels, tmp_labels), axis = 0)\n",
    "            gold_lengths = np.concatenate((gold_lengths, tmp_lengths), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTX2rLjkfw5d"
   },
   "source": [
    "5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "d20aWoHofv0W"
   },
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "permutation = np.random.permutation(gold_labels.shape[0])\n",
    "shuffled_content = contents[permutation, :, :]\n",
    "shuffled_labels = gold_labels[permutation]\n",
    "shuffled_length = gold_lengths[permutation]\n",
    "\n",
    "size = len(shuffled_labels)\n",
    "unit = int(size / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yMAFU_wMheTO"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "size = len(shuffled_labels)\n",
    "unit = int(size / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "fGtn4HaGpSLj",
    "outputId": "e63f47e7-9c3b-4334-b5a1-40370e35bc0b"
   },
   "outputs": [],
   "source": [
    "shuffled_labels\n",
    "K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "K13ZYsyamdJr"
   },
   "outputs": [],
   "source": [
    "# Calculate rate\n",
    "label_rate = []\n",
    "for i in range(0, 13):\n",
    "    label_rate.append(np.sum(shuffled_labels*(max_val-min_val)+min_val==i))\n",
    "label_rate = np.array(label_rate)\n",
    "label_rate = label_rate / (len(shuffled_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Xkw_DH24mbH_"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    dataset = np.concatenate((shuffled_content[0 : 5*unit], shuffled_content[0 : 4*unit]), axis=0) \n",
    "    label = np.concatenate((shuffled_labels[0 : 5*unit], shuffled_labels[0 : 4*unit]), axis=0) \n",
    "    length = np.concatenate((shuffled_length[0 : 5*unit], shuffled_length[0 : 4*unit]), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6kTI10dlpn0d"
   },
   "outputs": [],
   "source": [
    "np.save('/content/drive/My Drive/Semantic Model/data/dataset', dataset)\n",
    "np.save('/content/drive/My Drive/Semantic Model/data/label', label)\n",
    "np.save('/content/drive/My Drive/Semantic Model/data/length', length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-YpwxkV4P8vl"
   },
   "outputs": [],
   "source": [
    "dataset = np.load('/content/drive/My Drive/Semantic Model/data/dataset.npy')\n",
    "label = np.load('/content/drive/My Drive/Semantic Model/data/label.npy')\n",
    "length = np.load('/content/drive/My Drive/Semantic Model/data/length.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F4c3zL1wpPp3"
   },
   "outputs": [],
   "source": [
    "label_rate = np.array([0    , 0, 0.05284016, 0.05636284, 0.05239982,\n",
    "       0.05239982, 0.04843681, 0.05944518, 0.30250991, 0.14707177,\n",
    "       0.13914575, 0.04799648, 0.04139146])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5ZTHUbSYqJx_"
   },
   "outputs": [],
   "source": [
    "max_val = 12\n",
    "min_val = 2\n",
    "label_rates = []\n",
    "for i in range(len(label)):\n",
    "    label_rates.append(label_rate[int(label[i]*(max_val-min_val)+min_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CiNGf855uhhV"
   },
   "outputs": [],
   "source": [
    "label_rates = np.array(label_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dhJZpQK-QgfJ"
   },
   "outputs": [],
   "source": [
    "unit = int (len(label_rates) / 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PR3WntEGYGOF"
   },
   "outputs": [],
   "source": [
    "# 5 sets：each set --> 60% train dataset，20% development dataset，20% test dataset\n",
    "train_dataset = []\n",
    "train_label = []\n",
    "train_length = []\n",
    "train_rate = []\n",
    "dev_dataset = []\n",
    "dev_label = []\n",
    "dev_length = []\n",
    "dev_rate = []\n",
    "test_dataset = []\n",
    "test_label = []\n",
    "test_length = []\n",
    "test_rate = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_dataset.append([dataset[i*unit : (i+3)*unit]])\n",
    "    train_label.append([label[i*unit : (i+3)*unit]])\n",
    "    train_length.append([length[i*unit : (i+3)*unit]])\n",
    "    train_rate.append([label_rates[i*unit : (i+3)*unit]])\n",
    "    dev_dataset.append([dataset[(i+3)*unit : (i+4)*unit]])\n",
    "    dev_label.append([label[(i+3)*unit : (i+4)*unit]])\n",
    "    dev_length.append([length[(i+3)*unit : (i+4)*unit]])\n",
    "    dev_rate.append([label_rates[(i+3)*unit : (i+4)*unit]])\n",
    "    test_dataset.append([dataset[(i+4)*unit : (i+5)*unit]])\n",
    "    test_label.append([label[(i+4)*unit : (i+5)*unit]])\n",
    "    test_length.append([length[(i+4)*unit : (i+5)*unit]])\n",
    "    test_rate.append([label_rates[(i+4)*unit : (i+5)*unit]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOt2oZvs_TJd"
   },
   "source": [
    "Bi-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QSOy__AAxowa"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/ASAPAES')\n",
    "import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1C8ww2XRqLVU"
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, labels, batch_size, seq_lengths num_epochs):\n",
    "    \"\"\"\n",
    "    A mini-batch iterator to generate mini-batches for training neural network\n",
    "    param data: a list of sentences. each sentence is a vector of integers\n",
    "    param labels: a list of labels\n",
    "    param batch_size: the size of mini-batch\n",
    "    param num_epochs: number of epochs\n",
    "    return: a mini-batch iterator\n",
    "    \"\"\"\n",
    "    assert len(data) == len(labels) == len(seq_lengths)\n",
    "    data_size = len(data)\n",
    "\n",
    "    epoch_length = data_size // batch_size\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for i in range(epoch_length):\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "\n",
    "            xdata = data[start_index: end_index]\n",
    "            ydata = labels[start_index: end_index]\n",
    "            sequence_length = lengths[start_index: end_index]\n",
    "            # extra_rates = rates[start_index: end_index]\n",
    "            \n",
    "            permutation = np.random.permutation(xdata.shape[0])\n",
    "            xdata = xdata[permutation, :, :]\n",
    "            ydata = ydata[permutation]\n",
    "            sequence_length = sequence_length[permutation]\n",
    "            # extra_rates = extra_rates[permutation]\n",
    "            \n",
    "            yield xdata, ydata, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wsrtT6In7tFM"
   },
   "outputs": [],
   "source": [
    "class Bi_LSTM_Model(object):\n",
    "    def __init__(self):\n",
    "        # Score range： 2～12 \n",
    "        self.hidden_size = 1024\n",
    "        self.num_layers = 1\n",
    "        self.l2_reg_lambda = 0.001\n",
    "        \n",
    "        # Placeholders\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[], name='batch_size')\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, 68, 768], name='input_x')\n",
    "        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='keep_prob')\n",
    "        self.sequence_length = tf.placeholder(dtype=tf.int32, shape=[None], name='sequence_length')\n",
    "        self.rate = tf.placeholder(dtype=tf.float32, shape=[None], name='rate')\n",
    "        # L2 loss\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Word embedding\n",
    "        with tf.name_scope('embedding'):\n",
    "#             embedding = tf.get_variable('embedding',\n",
    "#                                         shape=[self.vocab_size, self.hidden_size],\n",
    "#                                         dtype=tf.float32)\n",
    "#             inputs = tf.nn.embedding_lookup(wv, self.input_x)\n",
    "\n",
    "            inputs = self.input_x\n",
    "\n",
    "\n",
    "        # Input dropout\n",
    "        self.inputs = tf.nn.dropout(inputs, keep_prob=self.keep_prob)\n",
    "\n",
    "        self.final_state = self.bi_lstm()\n",
    "        \n",
    "        # Softmax output layer\n",
    "        with tf.name_scope('sigmoid'):\n",
    "            # softmax_w = tf.get_variable('softmax_w', shape=[self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            sigmoid_w = tf.get_variable('sigmoid_w', shape=[2 * self.hidden_size, 1], dtype=tf.float32)\n",
    "            sigmoid_b = tf.get_variable('sigmoid_b', shape=[1], dtype=tf.float32)\n",
    "\n",
    "            # L2 regularization for output layer\n",
    "            self.l2_loss += tf.nn.l2_loss(sigmoid_w)\n",
    "            self.l2_loss += tf.nn.l2_loss(sigmoid_b)\n",
    "\n",
    "            # self.logits = tf.matmul(self.final_state[self.num_layers - 1].h, softmax_w) + softmax_b\n",
    "            # self.logits = tf.matmul(self.final_state, sigmoid_w) + sigmoid_b\n",
    "            # predictions = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.matmul(self.final_state, sigmoid_w) + sigmoid_b\n",
    "            self.predictions = tf.squeeze(self.predictions)\n",
    "            self.predictions = tf.nn.sigmoid(self.predictions, name='predictions')\n",
    "            # self.predictions = tf.argmax(predictions, 1, name='predictions')\n",
    "            # tf.assign(self.predictions, self.logits)\n",
    "\n",
    "        # Loss (MSE)\n",
    "        with tf.name_scope('loss'):\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            # L2 regularization for LSTM weights\n",
    "            for tv in tvars:\n",
    "                if 'kernel' in tv.name:\n",
    "                    self.l2_loss += tf.nn.l2_loss(tv)\n",
    "\n",
    "#             losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,\n",
    "#                                                                     logits=self.logits)\n",
    "            losses = tf.square((self.predictions - self.input_y)/self.rate)\n",
    "            self.cost = tf.reduce_mean(losses, name=\"loss\") + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(tf.round(self.predictions*(max_val-min_val)+min_val), self.input_y*(max_val-min_val)+min_val)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "\n",
    "\n",
    "    def bi_lstm(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                          forget_bias=1.0,\n",
    "                                          state_is_tuple=True,\n",
    "                                          reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                          forget_bias=1.0,\n",
    "                                          state_is_tuple=True,\n",
    "                                          reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "        # Add dropout to cell output\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.keep_prob)\n",
    "        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.keep_prob)\n",
    "\n",
    "        # Stacked LSTMs\n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell([cell_fw], state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell([cell_bw], state_is_tuple=True)\n",
    "\n",
    "        self._initial_state_fw = cell_fw.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self._initial_state_bw = cell_bw.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        # Dynamic Bi-LSTM\n",
    "        with tf.variable_scope('Bi-LSTM'):\n",
    "            _, state = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "                                                       cell_bw,\n",
    "                                                       inputs=self.inputs,\n",
    "                                                       initial_state_fw=self._initial_state_fw,\n",
    "                                                       initial_state_bw=self._initial_state_bw,\n",
    "                                                       sequence_length=self.sequence_length)\n",
    "\n",
    "        state_fw = state[0]\n",
    "        state_bw = state[1]\n",
    "        output = tf.concat([state_fw[self.num_layers - 1].h, state_bw[self.num_layers - 1].h], 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsyRl-YFp7gf"
   },
   "source": [
    "Semantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lUPHc9OHp8mm"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters (LSTMs are all single layer)\n",
    "hidden_size = 1024 # Number of hidden units in the LSTM cell\n",
    "keep_prob = 0.5 # Dropout keep probability\n",
    "learning_rate = 1e-5 \n",
    "l2_reg_lambda = 0.001 # L2 regularization lambda\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 256\n",
    "num_epochs = 60 # 100 epochs stops\n",
    "decay_rate = 1 \n",
    "decay_steps = 100000 # Learning rate decay rate. Range: (0, 1]\n",
    "save_every_steps = 100\n",
    "evaluate_every_steps = 10 # Evaluate the model on validation set after this many steps\n",
    "num_checkpoint = 50 # number of models to store\n",
    "\n",
    "for i in range(4, 5):\n",
    "    outdir = os.path.abspath(os.path.join(os.path.curdir, \"Semantic Model\", \"runs\", \"Bi-LSTM\", \"Model_\" + str(i)))\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # Load and save data\n",
    "    # =============================================================================\n",
    "\n",
    "    # Simple Cross validation\n",
    "    # Batch iterator\n",
    "    x_train = train_dataset[i][0]\n",
    "    y_train = train_label[i][0]\n",
    "    train_lengths = train_length[i][0]\n",
    "    train_rates = train_rate[i][0]\n",
    "    x_dev = dev_dataset[i][0]\n",
    "    y_dev = dev_label[i][0]\n",
    "    dev_lengths = dev_length[i][0]\n",
    "    dev_rates = dev_rate[i][0]\n",
    "    train_data = batch_iter(x_train, y_train, train_lengths, train_rates, batch_size, num_epochs)\n",
    "  \n",
    "    # Train\n",
    "    # =============================================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            classifier = Bi_LSTM_Model()\n",
    "            # Train procedure\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            # Learning rate decay\n",
    "            starter_learning_rate = learning_rate\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                                     global_step,\n",
    "                                                     decay_steps,\n",
    "                                                     decay_rate,\n",
    "                                                     staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Summaries\n",
    "            loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "            accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "            # Train summary\n",
    "            train_summary_op = tf.summary.merge_all()\n",
    "            train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Validation summary\n",
    "            valid_summary_op = tf.summary.merge_all()\n",
    "            valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "            valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "            saver = tf.train.Saver(max_to_keep=num_checkpoint)\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "            def run_step(input_data, is_training=True):\n",
    "                \"\"\"Run one step of the training process.\"\"\"\n",
    "                input_x, input_y, sequence_length, input_rate = input_data\n",
    "\n",
    "                fetches = {'step': global_step,\n",
    "                         'cost': classifier.cost,\n",
    "                         'accuracy': classifier.accuracy,\n",
    "                         'learning_rate': learning_rate}\n",
    "                feed_dict = {classifier.input_x: input_x,\n",
    "                           classifier.input_y: input_y,\n",
    "                          classifier.rate: input_rate}\n",
    "                fetches['final_state'] = classifier.final_state\n",
    "                fetches['predictions'] = classifier.predictions\n",
    "                feed_dict[classifier.batch_size] = len(input_x)\n",
    "                feed_dict[classifier.sequence_length] = sequence_length\n",
    "\n",
    "                if is_training:\n",
    "                    fetches['train_op'] = train_op\n",
    "                    fetches['summaries'] = train_summary_op\n",
    "                    feed_dict[classifier.keep_prob] = keep_prob\n",
    "                else:\n",
    "                    fetches['summaries'] = valid_summary_op\n",
    "                    feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "                vars = sess.run(fetches, feed_dict)\n",
    "                step = vars['step']\n",
    "                predictions = vars['predictions']\n",
    "                cost = vars['cost']\n",
    "                accuracy = vars['accuracy']\n",
    "                summaries = vars['summaries']\n",
    "\n",
    "                # Write summaries to file\n",
    "                if is_training:\n",
    "                    train_summary_writer.add_summary(summaries, step)\n",
    "                else:\n",
    "                    valid_summary_writer.add_summary(summaries, step)\n",
    "                    qwks = score.quadratic_weighted_kappa((predictions*10+2).astype(int), (input_y*10+2).astype(int), 2, 12)\n",
    "                    print (\"QWK \", qwks)\n",
    "\n",
    "                  \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step: {}, loss: {:g}, accuracy: {:g}\".format(time_str, step, cost, accuracy))\n",
    "\n",
    "                return accuracy\n",
    "\n",
    "\n",
    "            print('Start training ...')\n",
    "\n",
    "            for train_input in train_data:\n",
    "                run_step(train_input, is_training=True)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % evaluate_every_steps == 0:\n",
    "                    print('\\nDevlopment Set Validation')\n",
    "                    dev_data = batch_iter(x_dev, y_dev, dev_lengths, dev_rates, batch_size, 1)\n",
    "                    for dev_input in dev_data:\n",
    "                        run_step(dev_input, is_training=False)\n",
    "                    print('End Development Set Validation\\n')\n",
    "\n",
    "                if current_step % save_every_steps == 0:\n",
    "                    save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "\n",
    "            print('\\nAll the files have been saved to {}\\n'.format(outdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6hoX1e1nxdF6",
    "outputId": "8c22e079-0660-4518-ea68-8e251761fa94"
   },
   "outputs": [],
   "source": [
    "inference_graph = tf.Graph()\n",
    "with tf.Session(graph = inference_graph) as sess:\n",
    "    graph = tf.get_default_graph()\n",
    "    path = '/content/drive/My Drive/Semantic Model/runs/Bi-LSTM/'\n",
    "\n",
    "    for i in range(0,1):\n",
    "        print(\"No.\" + str(i) + \" Model\\n\")\n",
    "        bilstm_model = tf.train.import_meta_graph(os.path.join(path, \"Model_\"+str(i), \"model\", \"clf-300.meta\"))\n",
    "        bilstm_model.restore(sess, tf.train.latest_checkpoint(os.path.join(path, \"Model_\"+str(i), \"model\")))\n",
    "    \n",
    "        x_dev = test_dataset[i][0]\n",
    "        y_dev = test_label[i][0]\n",
    "        dev_lengths = test_length[i][0]\n",
    "        dev_rates = test_rate[i][0]\n",
    "\n",
    "        print('\\nDevlopment Set Validation ' + str(i))\n",
    "        dev_data = batch_iter(x_dev, y_dev, dev_lengths, dev_rates,256, 1)\n",
    "        for dev_input in dev_data:\n",
    "            x_ = inference_graph.get_tensor_by_name('input_x:0')\n",
    "            y_ = inference_graph.get_tensor_by_name('input_y:0')\n",
    "            rate_ = inference_graph.get_tensor_by_name('rate:0')\n",
    "            prediction_ = inference_graph.get_tensor_by_name('sigmoid/predictions:0')\n",
    "            keep_prob_ = inference_graph.get_tensor_by_name('keep_prob:0')\n",
    "            loss_ = inference_graph.get_tensor_by_name('loss/add_2:0')\n",
    "            accuracy_ = inference_graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "            sequence_length_ = inference_graph.get_tensor_by_name('sequence_length:0')\n",
    "            batch_size_ = inference_graph.get_tensor_by_name('batch_size:0')\n",
    "            vars = sess.run([accuracy_, loss_, prediction_], \n",
    "                      feed_dict={x_: dev_input[0],\n",
    "                                 y_: dev_input[1],\n",
    "                                 keep_prob_: 1.0,\n",
    "                                 sequence_length_: dev_input[2],\n",
    "                                 rate_: dev_input[3],\n",
    "                                 batch_size_ : 256})\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "            qwks = score.quadratic_weighted_kappa((vars[2]*10+2).astype(int), (dev_input[1]*10+2).astype(int), 2, 12)\n",
    "            print(\"qwks\",qwks,\" \", score.mean_quadratic_weighted_kappa([qwks]))\n",
    "            acc = (np.sum(((np.round(vars[2]*10+2)-(dev_input[1]*10+2))==0)==True)/len(vars[2]))\n",
    "            print(\"{}: loss: {:g}, accuracy: {:g}\".format(time_str, vars[1], acc))\n",
    "            print('End Development Set Validation ' + str(i) +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGe3g0WGqNlD"
   },
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AqjiJzhpASIV"
   },
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "permutation = np.random.permutation(gold_labels.shape[0])\n",
    "shuffled_content = contents[permutation, :, :]\n",
    "shuffled_labels = gold_labels[permutation]\n",
    "shuffled_length = gold_lengths[permutation]\n",
    "\n",
    "size = len(shuffled_labels)\n",
    "unit = int(size / 5)\n",
    "\n",
    "train_dataset = shuffled_content[: unit * 4]\n",
    "train_labels = shuffled_labels[: unit * 4]\n",
    "seq_lengths = shuffled_length[: unit * 4]\n",
    "test_dataset = shuffled_content[unit * 4 : ]\n",
    "test_labels = shuffled_labels[unit * 4 :]\n",
    "seq_lengths = shuffled_length[unit * 4 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6tYoRzYlqMdW"
   },
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self):\n",
    "        # 分数段： 2～12 \n",
    "        self.hidden_size = 1024\n",
    "        self.num_layers = 1\n",
    "        self.l2_reg_lambda = 0.001\n",
    "        \n",
    "        # Placeholders\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[], name='batch_size')\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, 128, 768], name='input_x')\n",
    "        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='keep_prob')\n",
    "        self.sequence_length = tf.placeholder(dtype=tf.int32, shape=[None], name='sequence_length')\n",
    "        # self.rate = tf.placeholder(dtype=tf.float32, shape=[None], name='rate')\n",
    "        # L2 loss\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Word embedding\n",
    "        with tf.name_scope('embedding'):\n",
    "#             embedding = tf.get_variable('embedding',\n",
    "#                                         shape=[self.vocab_size, self.hidden_size],\n",
    "#                                         dtype=tf.float32)\n",
    "#             inputs = tf.nn.embedding_lookup(wv, self.input_x)\n",
    "\n",
    "            inputs = self.input_x\n",
    "\n",
    "\n",
    "        # Input dropout\n",
    "        self.inputs = tf.nn.dropout(inputs, keep_prob=self.keep_prob)\n",
    "\n",
    "        self.final_state = self.lstm()\n",
    "        \n",
    "        # Softmax output layer\n",
    "        with tf.name_scope('sigmoid'):\n",
    "            # softmax_w = tf.get_variable('softmax_w', shape=[self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            sigmoid_w = tf.get_variable('sigmoid_w', shape=[self.hidden_size, 1], dtype=tf.float32)\n",
    "            sigmoid_b = tf.get_variable('sigmoid_b', shape=[1], dtype=tf.float32)\n",
    "\n",
    "            # L2 regularization for output layer\n",
    "            self.l2_loss += tf.nn.l2_loss(sigmoid_w)\n",
    "            self.l2_loss += tf.nn.l2_loss(sigmoid_b)\n",
    "\n",
    "            self.predictions = tf.matmul(self.final_state, sigmoid_w) + sigmoid_b\n",
    "            self.predictions = tf.squeeze(self.predictions)\n",
    "            self.predictions = tf.nn.sigmoid(self.predictions, name='predictions') \n",
    "\n",
    "\n",
    "        # Loss (MSE)\n",
    "        with tf.name_scope('loss'):\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            # L2 regularization for LSTM weights\n",
    "            for tv in tvars:\n",
    "                if 'kernel' in tv.name:\n",
    "                    self.l2_loss += tf.nn.l2_loss(tv)\n",
    "\n",
    "            losses = tf.square((self.predictions - self.input_y)/self.rate)\n",
    "            self.cost = tf.reduce_mean(losses, name=\"loss\") + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(tf.round(self.predictions*(max_val-min_val)+min_val), self.input_y*(max_val-min_val)+min_val)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "\n",
    "\n",
    "    def lstm(self):\n",
    "\n",
    "        cell = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                          forget_bias= 1.0,\n",
    "                                          state_is_tuple=True,\n",
    "                                          reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "\n",
    "        # Add dropout to cell output\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "\n",
    "        # Stacked LSTMs\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell], state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        # Dynamic LSTM\n",
    "        with tf.variable_scope('LSTM'):\n",
    "            _, state = tf.nn.dynamic_rnn(cell,\n",
    "                                         inputs=self.inputs,\n",
    "                                         initial_state=self._initial_state,\n",
    "                                         sequence_length=self.sequence_length)\n",
    "\n",
    "        output = state[self.num_layers - 1].h\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YxJyX643AplC"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters (LSTMs are all single layer)\n",
    "hidden_size = 1024 # Number of hidden units in the LSTM cell\n",
    "keep_prob = 0.5 # Dropout keep probability\n",
    "learning_rate = 1e-5 \n",
    "l2_reg_lambda = 0.001 # L2 regularization lambda\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 80 # 100 epochs\n",
    "decay_rate = 1 \n",
    "decay_steps = 100000 # Learning rate decay rate. Range: (0, 1]\n",
    "save_every_steps = 100\n",
    "evaluate_every_steps = 10 # Evaluate the model on validation set after this many steps\n",
    "num_checkpoint = 50 # number of models to store\n",
    "\n",
    "\n",
    "# Train\n",
    "# =============================================================================\n",
    "train_data = batch_iter(train_dataset, train_labels, seq_lengths, batch_size, num_epochs)\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        classifier = LSTM_Model()\n",
    "        # Train procedure\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # Learning rate decay\n",
    "        starter_learning_rate = learning_rate\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                                    global_step,\n",
    "                                                    decay_steps,\n",
    "                                                    decay_rate,\n",
    "                                                    staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Summaries\n",
    "        loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "        accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "        # Train summary\n",
    "        train_summary_op = tf.summary.merge_all()\n",
    "        train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Validation summary\n",
    "        valid_summary_op = tf.summary.merge_all()\n",
    "        valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "        valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=num_checkpoint)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def run_step(input_data, is_training=True):\n",
    "            \"\"\"Run one step of the training process.\"\"\"\n",
    "            input_x, input_y = input_data\n",
    "\n",
    "            fetches = {'step': global_step,\n",
    "                        'cost': classifier.cost,\n",
    "                        'accuracy': classifier.accuracy,\n",
    "                        'learning_rate': learning_rate}\n",
    "            feed_dict = {classifier.input_x: input_x,\n",
    "                          classifier.input_y: input_y,\n",
    "                         classifier.seq_lengths: seq_lengths}\n",
    "            fetches['final_state'] = classifier.final_state\n",
    "            fetches['predictions'] = classifier.predictions\n",
    "            feed_dict[classifier.batch_size] = len(input_x)\n",
    "            # feed_dict[classifier.sequence_length] = sequence_length\n",
    "\n",
    "            if is_training:\n",
    "                fetches['train_op'] = train_op\n",
    "                fetches['summaries'] = train_summary_op\n",
    "                feed_dict[classifier.keep_prob] = keep_prob\n",
    "            else:\n",
    "                fetches['summaries'] = valid_summary_op\n",
    "                feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "            vars = sess.run(fetches, feed_dict)\n",
    "            step = vars['step']\n",
    "            cost = vars['cost']\n",
    "            predictions = vars['predictions']\n",
    "            accuracy = vars['accuracy']\n",
    "            summaries = vars['summaries']\n",
    "\n",
    "            # Write summaries to file\n",
    "            if is_training:\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "            else:\n",
    "                # qwks = score.quadratic_weighted_kappa((predictions*10+2).astype(int), (input_y*10+2).astype(int), 2, 12)\n",
    "                # print (\"QWK \", qwks)\n",
    "                valid_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step: {}, loss: {:g}, accuracy: {:g}\".format(time_str, step, cost, accuracy))\n",
    "\n",
    "            return accuracy\n",
    "\n",
    "\n",
    "        print('Start training ...')\n",
    "\n",
    "        for train_input in train_data:\n",
    "            run_step(train_input, is_training=True)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if current_step % evaluate_every_steps == 0:\n",
    "                print('\\nDevlopment Set Validation')\n",
    "                dev_data = batch_iter(x_dev, y_dev, seq_lengths_dev, batch_size, 1)\n",
    "                for dev_input in dev_data:\n",
    "                  run_step(dev_input, is_training=False)\n",
    "                print('End Development Set Validation\\n')\n",
    "\n",
    "            if current_step % save_every_steps == 0:\n",
    "                save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "\n",
    "        print('\\nAll the files have been saved to {}\\n'.format(outdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "P7WQuVwIenpE"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters (LSTMs are all single layer)\n",
    "hidden_size = 1024 # Number of hidden units in the LSTM cell\n",
    "keep_prob = 0.5 # Dropout keep probability\n",
    "learning_rate = 1e-5  \n",
    "l2_reg_lambda = 0.001 # L2 regularization lambda\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 80 # 100 epochs \n",
    "decay_rate = 1 \n",
    "decay_steps = 100000 # Learning rate decay rate. Range: (0, 1]\n",
    "save_every_steps = 100\n",
    "evaluate_every_steps = 10 # Evaluate the model on validation set after this many steps\n",
    "num_checkpoint = 50 # number of models to store\n",
    "\n",
    "# Output files directory\n",
    "for i in range(4,5):\n",
    "    outdir = os.path.abspath(os.path.join(os.path.curdir, \"Semantic Model\", \"runs\", \"LSTM\", \"Model_\" + str(i)))\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # Load and save data\n",
    "    # =============================================================================\n",
    "\n",
    "    # Simple Cross validation\n",
    "    # Batch iterator\n",
    "    x_train = train_dataset[i][0]\n",
    "    y_train = train_label[i][0]\n",
    "    train_lengths = train_length[i][0]\n",
    "    train_rates = train_rate[i][0]\n",
    "    x_dev = dev_dataset[i][0]\n",
    "    y_dev = dev_label[i][0]\n",
    "    dev_lengths = dev_length[i][0]\n",
    "    dev_rates = dev_rate[i][0]\n",
    "  \n",
    "    train_data = batch_iter(x_train, y_train, train_lengths, train_rates, batch_size, num_epochs)\n",
    "  \n",
    "    # Train\n",
    "    # =============================================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            classifier = LSTM_Model()\n",
    "            # Train procedure\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            # Learning rate decay\n",
    "            starter_learning_rate = learning_rate\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                                     global_step,\n",
    "                                                     decay_steps,\n",
    "                                                     decay_rate,\n",
    "                                                     staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Summaries\n",
    "            loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "            accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "            # Train summary\n",
    "            train_summary_op = tf.summary.merge_all()\n",
    "            train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Validation summary\n",
    "            valid_summary_op = tf.summary.merge_all()\n",
    "            valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "            valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "            saver = tf.train.Saver(max_to_keep=num_checkpoint)\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "            def run_step(input_data, is_training=True):\n",
    "            \"\"\"Run one step of the training process.\"\"\"\n",
    "                input_x, input_y, sequence_length, input_rate = input_data\n",
    "\n",
    "                fetches = {'step': global_step,\n",
    "                         'cost': classifier.cost,\n",
    "                         'accuracy': classifier.accuracy,\n",
    "                         'learning_rate': learning_rate}\n",
    "                feed_dict = {classifier.input_x: input_x,\n",
    "                           classifier.input_y: input_y,\n",
    "                          classifier.rate: input_rate}\n",
    "                fetches['final_state'] = classifier.final_state\n",
    "                fetches['predictions'] = classifier.predictions\n",
    "                feed_dict[classifier.batch_size] = len(input_x)\n",
    "                feed_dict[classifier.sequence_length] = sequence_length\n",
    "\n",
    "                if is_training:\n",
    "                    fetches['train_op'] = train_op\n",
    "                    fetches['summaries'] = train_summary_op\n",
    "                    feed_dict[classifier.keep_prob] = keep_prob\n",
    "                else:\n",
    "                    fetches['summaries'] = valid_summary_op\n",
    "                    feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "                vars = sess.run(fetches, feed_dict)\n",
    "                step = vars['step']\n",
    "                cost = vars['cost']\n",
    "                predictions = vars['predictions']\n",
    "                accuracy = vars['accuracy']\n",
    "                summaries = vars['summaries']\n",
    "\n",
    "                # Write summaries to file\n",
    "                if is_training:\n",
    "                    train_summary_writer.add_summary(summaries, step)\n",
    "                else:\n",
    "                    qwks = score.quadratic_weighted_kappa((predictions*10+2).astype(int), (input_y*10+2).astype(int), 2, 12)\n",
    "                    print (\"QWK \", qwks)\n",
    "                    valid_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step: {}, loss: {:g}, accuracy: {:g}\".format(time_str, step, cost, accuracy))\n",
    "\n",
    "                return accuracy\n",
    "\n",
    "\n",
    "            print('Start training ...')\n",
    "\n",
    "            for train_input in train_data:\n",
    "                run_step(train_input, is_training=True)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                if current_step % evaluate_every_steps == 0:\n",
    "                    print('\\nDevlopment Set Validation')\n",
    "                    dev_data = batch_iter(x_dev, y_dev, dev_lengths, dev_rates, batch_size, 1)\n",
    "                    for dev_input in dev_data:\n",
    "                        run_step(dev_input, is_training=False)\n",
    "                    print('End Development Set Validation\\n')\n",
    "\n",
    "                if current_step % save_every_steps == 0:\n",
    "                    save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "\n",
    "            print('\\nAll the files have been saved to {}\\n'.format(outdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2992
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "VEJDbA9xV3Yo",
    "outputId": "3e275416-42e4-48b7-f5ab-1c6e48dfa211"
   },
   "outputs": [],
   "source": [
    "inference_graph = tf.Graph()\n",
    "with tf.Session(graph = inference_graph) as sess:\n",
    "  \n",
    "    graph = tf.get_default_graph()\n",
    "    path = '/content/drive/My Drive/Semantic Model/runs/LSTM/'\n",
    "\n",
    "  \n",
    "    for i in range(3,4):\n",
    "        print(\"No.\" + str(i) + \" Model\\n\")\n",
    "        bilstm_model = tf.train.import_meta_graph(os.path.join(path, \"Model_\"+str(i), \"model\", \"clf-400.meta\"))\n",
    "        bilstm_model.restore(sess, tf.train.latest_checkpoint(os.path.join(path, \"Model_\"+str(i), \"model\")))\n",
    "    \n",
    "        x_dev = train_dataset[i][0]\n",
    "        y_dev = train_label[i][0]\n",
    "        dev_lengths = train_length[i][0]\n",
    "        dev_rates = train_rate[i][0]\n",
    "\n",
    "        print('\\nDevlopment Set Validation ' + str(i))\n",
    "        dev_data = batch_iter(x_dev, y_dev, dev_lengths, dev_rates,256, 1)\n",
    "        for dev_input in dev_data:\n",
    "      \n",
    "        x_ = inference_graph.get_tensor_by_name('input_x:0')\n",
    "        y_ = inference_graph.get_tensor_by_name('input_y:0')\n",
    "        rate_ = inference_graph.get_tensor_by_name('rate:0')\n",
    "        prediction_ = inference_graph.get_tensor_by_name('sigmoid/predictions:0')\n",
    "        keep_prob_ = inference_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loss_ = inference_graph.get_tensor_by_name('loss/loss:0')\n",
    "        accuracy_ = inference_graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "        sequence_length_ = inference_graph.get_tensor_by_name('sequence_length:0')\n",
    "        batch_size_ = inference_graph.get_tensor_by_name('batch_size:0')\n",
    "        vars = sess.run([accuracy_, loss_, prediction_], \n",
    "                      feed_dict={x_: dev_input[0],\n",
    "                                 y_: dev_input[1],\n",
    "                                 keep_prob_: 1.0,\n",
    "                                 sequence_length_: dev_input[2],\n",
    "                                 rate_: dev_input[3],\n",
    "                                 batch_size_ : 256})\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(np.round(vars[2]*10+2))\n",
    "        print (dev_input[1]*10+2)\n",
    "        qwks = score.quadratic_weighted_kappa(np.round((vars[2]*10+2)).astype(int), (dev_input[1]*10+2).astype(int), 2, 12)\n",
    "        print(\"qwks\",qwks,\" \", score.mean_quadratic_weighted_kappa([qwks]))\n",
    "        acc = np.sum(((np.round(vars[2]*10+2)-(dev_input[1]*10+2))==0)==True) / len(vars[2])\n",
    "        print(\"{}: loss: {:g}, accuracy: {:g}\".format(time_str, vars[1], acc))\n",
    "        print('End Development Set Validation ' + str(i) +'\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Network Semantic Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
